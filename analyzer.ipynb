{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.83:7077\") \\\n",
    "        .appName(\"Aram_testar\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",False)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\",2)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "#If you get following error: \"WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\" \n",
    "#solve by setting dynamicAlloc and Shuffletracking to FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read genres.csv \n",
    "schema = StructType([\\\n",
    "    StructField(\"TrackID\", StringType(), True),\\\n",
    "    StructField(\"Genre\", StringType(), True)])\n",
    "\n",
    "million_song_df = spark.read.csv(\"hdfs://192.168.2.83:9000/user/ubuntu/SongCSVWithTrackID.csv\", header=True, inferSchema=True)\n",
    "million_song_notbyte_df = million_song_df.map()\n",
    "genres_df = spark.read.option(\"delimiter\", \"\\t\").csv(\"hdfs://192.168.2.83:9000/user/ubuntu/genres.csv\", schema=schema)\n",
    "\n",
    "# Filter out songs with year=0\n",
    "filtered_million_song_df = million_song_df.filter(col(\"year\") != 0)\n",
    "\n",
    "# Join the two DataFrames on the common column (e.g., track_id)\n",
    "joined_df = genres_df.join(filtered_million_song_df, genres_df[\"TrackID\"] == million_song_df[\"TrackID\"])\n",
    "\n",
    "# Group by genre and year, then calculate the percentage of songs in each genre by year\n",
    "# result_df = joined_df.groupBy(\"Genre\", \"year\").count()\n",
    "\n",
    "\n",
    "# Show the result or save it as needed\n",
    "joined_df.take(1)\n",
    "\n",
    "\n",
    "#problem: how to read h5 files\n",
    "#maybe read in as pandas and then convert to Spark dataframe??\n",
    "#Jag vill göra så att koden går igenom alla subdir i millionsongsubset och tar fram namn på alla .h5 filer. De läses sedan in som Pandas->dataframe elr ngt sånt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release the cores for another application!\n",
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(SongNumber=1, TrackID=\"b'TRAZZOQ12903CDDCB6'\", AlbumID=816192, AlbumName=\"b'Kingdom Of Ghosts'\", ArtistID=\"b'AR3OYPP1187B9952E3'\", ArtistLatitude=53.34376, ArtistLocation=\"b'Dublin Ireland'\", ArtistLongitude=-6.24953, ArtistName=\"b'Humanzi'\", Danceability=0.0, Duration=242.99057, KeySignature=8, KeySignatureConfidence=0.496, Tempo=120.948, TimeSignature=4, TimeSignatureConfidence=0.854, Title=\"b'Amsterdamaged'\", Year=0)]\n",
      "[Row(TrackID='TRAAAAK128F9318786', Genre='Rock')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(songs_df.take(1))\n",
    "print(genre_df.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(TrackID='TRAAAAK128F9318786', Genre='Rock')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
