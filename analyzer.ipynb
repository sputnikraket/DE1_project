{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.148:7077\") \\\n",
    "        .appName(\"Aram_StrongScaling_largerDATA\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", False)\\\n",
    "        .config(\"spark.driver.port\", 9999)\\\n",
    "        .config(\"spark.blockManager.port\", 10005)\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\\\n",
    "    StructField(\"TrackID\", StringType(), True),\\\n",
    "    StructField(\"Genre\", StringType(), True)])\n",
    "\n",
    "million_song_df = spark.read.csv(\"hdfs://192.168.2.148:9000/user/ubuntu/SongCSVWithTrackID.csv\", header=True, inferSchema=True)\n",
    "\n",
    "genres_df = spark.read.option(\"delimiter\", \"\\t\").csv(\"hdfs://192.168.2.148:9000/user/ubuntu/genres.csv\", schema=schema)\n",
    "\n",
    "million_song_notbyte_df = million_song_df.withColumn(\"TrackID\", regexp_replace(col(\"TrackID\").cast(StringType()), \"['b']\", \"\"))\n",
    "\n",
    "\n",
    "million_song_notbyte_df.take(1)\n",
    "# Filter out songs with year=0\n",
    "filtered_million_song_df = million_song_notbyte_df.filter(col(\"Year\") != 0)\n",
    "\n",
    "# Join the two DataFrames on the common column (e.g., track_id)\n",
    "# joined_df = genres_df.join(filtered_million_song_df, genres_df[\"TrackID\"] == million_song_df[\"TrackID\"])\n",
    "joined_df = genres_df.join(filtered_million_song_df, genres_df[\"TrackID\"] == filtered_million_song_df[\"TrackID\"])\n",
    "\n",
    "joined_pandas = joined_df.toPandas()\n",
    "joined_pandas.to_csv(\"Data_to_replicate.csv\") #This is to be replciated to enlarge dataset. use \"data_replicator.py\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for larger data set STRONG SCALING\n",
    "million_song_df = spark.read.csv(\"hdfs://192.168.2.148:9000/user/ubuntu/replicated_Data_x2000.csv\", header=True, inferSchema=True)\n",
    "start_time = time.time()\n",
    "joined_df = million_song_df.withColumn(\"TrackID\", regexp_replace(col(\"TrackID\").cast(StringType()), \"['b']\", \"\"))\n",
    "\n",
    "# Group by genre and year, then calculate the percentage of songs in each genre by year\n",
    "result_df = joined_df.groupBy(\"Genre\", \"Year\").count()\n",
    "result_df = result_df.orderBy(\"Year\")\n",
    "pivot_table = result_df.groupBy(\"Year\").pivot(\"Genre\").agg(count(\"Genre\").alias(\"count\")).fillna(0).orderBy(\"Year\")\n",
    "\n",
    "pivot_table = result_df\n",
    "\n",
    "\n",
    "# Show the result or save it as needed\n",
    "# filtered_million_song_df.take(1)\n",
    "# joined_df.take(10)\n",
    "# result_df.take(100)\n",
    "\n",
    "# Group by Genre and Year and calculate the count\n",
    "result_df = joined_df.groupBy(\"Genre\", \"Year\").count()       \n",
    "\n",
    "# Pivot the DataFrame to display counts for each genre in each year\n",
    "pivot_result_df = result_df.groupBy(\"Year\").pivot(\"Genre\").agg(F.first(\"count\"))\n",
    "\n",
    "# Fill null values with 0\n",
    "pivot_result_df    =    pivot_result_df.na.fill(0).orderBy(\"Year\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Elapsed time: {end_time-start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 20.502609491348267 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#WEAK SCALING\n",
    "#change file to analyze manually ...x500... etc\n",
    "million_song_df = spark.read.csv(\"hdfs://192.168.2.148:9000/user/ubuntu/replicated_Data_x2500.csv\", header=True, inferSchema=True)\n",
    "start_time = time.time()\n",
    "joined_df = million_song_df.withColumn(\"TrackID\", regexp_replace(col(\"TrackID\").cast(StringType()), \"['b']\", \"\"))\n",
    "\n",
    "# Group by genre and year, then calculate the percentage of songs in each genre by year\n",
    "result_df = joined_df.groupBy(\"Genre\", \"Year\").count()\n",
    "result_df = result_df.orderBy(\"Year\")\n",
    "pivot_table = result_df.groupBy(\"Year\").pivot(\"Genre\").agg(count(\"Genre\").alias(\"count\")).fillna(0).orderBy(\"Year\")\n",
    "\n",
    "pivot_table = result_df\n",
    "\n",
    "# Group by Genre and Year and calculate the count\n",
    "result_df = joined_df.groupBy(\"Genre\", \"Year\").count()       \n",
    "\n",
    "# Pivot the DataFrame to display counts for each genre in each year\n",
    "pivot_result_df = result_df.groupBy(\"Year\").pivot(\"Genre\").agg(F.first(\"count\"))\n",
    "\n",
    "# Fill null values with 0\n",
    "pivot_result_df = pivot_result_df.na.fill(0).orderBy(\"Year\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Elapsed time: {end_time-start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_reslts_Pandas = pivot_result_df.toPandas()\n",
    "pivot_reslts_Pandas = pivot_reslts_Pandas.set_index(\"Year\")\n",
    "# pivot_reslts_Pandas = pivot_reslts_Pandas.apply(lambda x: x*100/sum(x), axis=1)\n",
    "pivot_reslts_Pandas_test = pivot_reslts_Pandas.div(pivot_reslts_Pandas.sum(axis=1), axis=0) * 100\n",
    "interestingDF=pivot_reslts_Pandas_test[pivot_reslts_Pandas_test.index > 1970]\n",
    "# interestingDF.plot(kind=\"bar\", stacked=True, figsize=(20,20))\n",
    "\n",
    "#save figure (will be on driver VM)\n",
    "interestingDF.plot(kind=\"bar\", stacked=True, figsize=(20,20)).get_figure().savefig(\"prelresult.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#release cores/stop app\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
